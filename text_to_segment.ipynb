{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleptonic/Text-To-Segment/blob/main/text_to_segment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Driven Image Segmentation with SAM 2\n",
        "\n",
        "This notebook implements a complete pipeline to perform segmentation on any object in an image using a free-form text prompt. The project's goal is to demonstrate how two different state-of-the-art models, **GroundingDINO** and the **Segment Anything Model (SAM) 2**, can be composed to achieve a powerful new capability."
      ],
      "metadata": {
        "id": "IXOePz5O4kvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency installation\n",
        "This cell installs all necessary libraries and downloads the pre-trained model weights.\n",
        "- **`transformers`**: From Hugging Face, used for a reliable and easy-to-use implementation of GroundingDINO.\n",
        "- **`segment-anything-py`**: The official library for the Segment Anything Model.\n",
        "- **`supervision`**: A helpful library for visualization and handling annotations (used in some helper functions)."
      ],
      "metadata": {
        "id": "wCzSHpIIEnp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers supervision segment-anything-py Pillow"
      ],
      "metadata": {
        "id": "d1Nx_nEq4mHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency"
      ],
      "metadata": {
        "id": "CpegnHN34rJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from dataclasses import dataclass\n",
        "# --- Download SAM weights ---\n",
        "WEIGHTS_DIR = os.path.join(os.getcwd(), \"weights\")\n",
        "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
        "SAM_WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, \"sam_vit_h_4b8939.pth\")\n",
        "\n",
        "if not os.path.exists(SAM_WEIGHTS_PATH):\n",
        "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {WEIGHTS_DIR}\n",
        "\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "print(\"✅ Initial Setup complete.\")\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import PIL.Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki-ZZoyG4pe6",
        "outputId": "f1c03396-8250-4ec2-f826-4a0e3a0ae5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Initial Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelParameters:\n",
        "  dino_model_id :str = \"IDEA-Research/grounding-dino-tiny\"\n",
        "  sam_model_type = \"vit_h\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "@dataclass\n",
        "class DataParameters:\n",
        "  img_path :str\n",
        "  vid_path :str\n",
        "  save_path :str\n",
        "  input_text_prompt :str\n",
        "\n"
      ],
      "metadata": {
        "id": "KikLSSc6475A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "The solution is built on a \"Finder-Painter\" principle, where each model has a specialized role:\n",
        "\n",
        "1.  **The Finder (GroundingDINO):** An open-set object detector that excels at understanding language. It takes our text prompt and finds *where* the object is in the image, returning bounding boxes.\n",
        "2.  **The Painter (SAM):** A promptable segmentation model that excels at understanding object boundaries. It takes the bounding boxes from the Finder as a guide and \"paints\" a pixel-perfect mask.\n",
        "\n",
        "Our code encapsulates these roles in distinct classes (`DINO`, `SAM`) and orchestrates the flow of data between them."
      ],
      "metadata": {
        "id": "8AXGsl1fDqeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DINO:\n",
        "  def __init__(self, mparams):\n",
        "    self.device = mparams.device\n",
        "    self.processor = AutoProcessor.from_pretrained(mparams.dino_model_id)\n",
        "    self.model = AutoModelForZeroShotObjectDetection.from_pretrained(mparams.dino_model_id)\n",
        "    print(\"GroundingDINO model loaded.\")\n",
        "\n",
        "  def _enhance_input_text(self, text_prompt:str):\n",
        "    \"\"\"To enhance the text prompt such that each probable class is seperated by '.' \"\"\"\n",
        "    prompt_list = [p.strip() for p in text_prompt.split(',')]\n",
        "    return prompt_list\n",
        "\n",
        "  def _get_bounding_box(self, image:PIL.Image, text_prompt):\n",
        "    inputs = self.processor(images=image, text=self._enhance_input_text(text_prompt), return_tensors=\"pt\").to(self.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "    results = self.processor.post_process_grounded_object_detection(\n",
        "        outputs,\n",
        "        inputs.input_ids,\n",
        "        threshold=0.4,\n",
        "        text_threshold=0.3,\n",
        "        target_sizes=[image.size[::-1]] # change to h,w\n",
        "    )\n",
        "\n",
        "    # Extract the boxes and labels\n",
        "    dino_boxes = results[0][\"boxes\"] # Get the boxes tensor\n",
        "    dino_labels = results[0][\"text_labels\"]\n",
        "    print(f\"GroundingDINO found labels: {dino_labels}\")\n",
        "    print(f\"GroundingDINO found {len(dino_boxes)} box(es).\")\n",
        "    return dino_boxes, dino_labels\n",
        "\n",
        "class SAM:\n",
        "  def __init__ (self, mparams):\n",
        "    self.device = mparams.device\n",
        "    self.sam = sam_model_registry[mparams.sam_model_type](checkpoint=SAM_WEIGHTS_PATH)\n",
        "    self.sam.to(device=mparams.device)\n",
        "    self.sam_predictor = SamPredictor(self.sam)\n",
        "    print(\"SAM model loaded.\")\n",
        "\n",
        "  def _segment(self, image:PIL.Image, bounding_boxes):\n",
        "    image_np = np.array(image)\n",
        "    self.sam_predictor.set_image(image_np)\n",
        "\n",
        "    # Transform the boxes from DINO to match SAM's internal image representation\n",
        "    transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(bounding_boxes, image_np.shape[:2])\n",
        "\n",
        "    # using transformed boxes for predictions\n",
        "    masks, _, _ = self.sam_predictor.predict_torch(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        boxes=transformed_boxes.to(self.device), # Use the transformed boxes\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    print(\"SAM generated masks.\")\n",
        "    return masks\n",
        "\n",
        "class Visualize:\n",
        "  @staticmethod\n",
        "  def _show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6]) # Dodger blue\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "  @staticmethod\n",
        "  def _show_box(box, ax, label):\n",
        "    x_min, y_min, x_max, y_max = box\n",
        "    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2,\n",
        "                             edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x_min, y_min - 10, label, color='white', fontsize=12,\n",
        "            bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "  @staticmethod\n",
        "  def _show_segmentation(image:PIL.Image, input_prompt:str, seg_masks, bounding_boxes, box_labels):\n",
        "    fig, ax = plt.subplots(figsize=(12, 12))\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(f\"Segmentation for: '{input_prompt}'\")\n",
        "\n",
        "    # Plotting masks\n",
        "    if seg_masks is not None:\n",
        "        for mask in seg_masks:\n",
        "            Visualize._show_mask(mask.cpu().numpy(), ax, random_color=True)\n",
        "    if len(bounding_boxes) > 0:\n",
        "        for box, label in zip(bounding_boxes, box_labels):\n",
        "            Visualize._show_box(box.cpu().numpy(), ax, label) # Pass the specific label for this box\n",
        "\n",
        "    plt.savefig('segmentation_mask.png')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T5IO8zak5jpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Demonstration\n",
        "\n",
        "\n",
        "- **Upload an image** from your local machine.\n",
        "- **Enter a text prompt** describing the object you want to segment (you can list multiple objects separated by commas, e.g., \"a person, a surfboard\").\n",
        "- **Click \"Run Segmentation\"** to see the final result."
      ],
      "metadata": {
        "id": "wKHiem4nDud1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UI_Handler:\n",
        "    def __init__(self, dino_model, sam_model):\n",
        "        \"\"\"\n",
        "        Initializes the handler with pre-loaded DINO and SAM models.\n",
        "        \"\"\"\n",
        "        self.dino = dino_model\n",
        "        self.sam = sam_model\n",
        "        self.image_path = None\n",
        "\n",
        "        # --- Create UI Widgets ---\n",
        "        self.uploader = widgets.FileUpload(\n",
        "            accept='image/*',\n",
        "            description='Upload Image'\n",
        "        )\n",
        "        self.text_prompt = widgets.Text(\n",
        "            value='a red cap',\n",
        "            placeholder='Enter object(s) to find, separated by commas',\n",
        "            description='Prompt:',\n",
        "            disabled=True # Disabled until an image is uploaded\n",
        "        )\n",
        "        self.run_button = widgets.Button(\n",
        "            description='Run Segmentation',\n",
        "            disabled=True,\n",
        "            button_style='success',\n",
        "            icon='check'\n",
        "        )\n",
        "        self.output = widgets.Output()\n",
        "\n",
        "        # --- Define Widget Actions ---\n",
        "        self.uploader.observe(self._handle_upload, names='value')\n",
        "        self.run_button.on_click(self._run_pipeline)\n",
        "\n",
        "    def _handle_upload(self, change):\n",
        "        \"\"\"Called when a file is uploaded.\"\"\"\n",
        "        with self.output:\n",
        "            clear_output() # Clear previous image/results\n",
        "            uploaded_files = change['new']\n",
        "\n",
        "            if not change['new']:\n",
        "                print(\"Upload cancelled.\")\n",
        "                return\n",
        "\n",
        "            # Get the uploaded file\n",
        "            uploaded_file = list(uploaded_files.values())[0]\n",
        "            filename = uploaded_file['metadata']['name']\n",
        "            content = uploaded_file['content']\n",
        "\n",
        "            # Define a path to save the image in the Colab runtime\n",
        "            self.image_path = f\"/content/{filename}\"\n",
        "\n",
        "            # Save the file\n",
        "            with open(self.image_path, 'wb') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            print(f\"Image '{filename}' uploaded successfully.\")\n",
        "\n",
        "            # Display the uploaded image\n",
        "            img = PIL.Image.open(self.image_path)\n",
        "            display(img)\n",
        "\n",
        "            # Enable the other widgets\n",
        "            self.text_prompt.disabled = False\n",
        "            self.run_button.disabled = False\n",
        "\n",
        "    def _run_pipeline(self, b):\n",
        "        \"\"\"Called when the 'Run Segmentation' button is clicked.\"\"\"\n",
        "        with self.output:\n",
        "            # Clear previous results but keep the uploaded image\n",
        "            clear_output(wait=True)\n",
        "            if self.image_path:\n",
        "                img_display = PIL.Image.open(self.image_path)\n",
        "                display(img_display)\n",
        "\n",
        "            prompt_text = self.text_prompt.value\n",
        "            if not prompt_text:\n",
        "                print(\"Error: Please enter a text prompt.\")\n",
        "                return\n",
        "\n",
        "            print(\"Processing... This may take a moment.\")\n",
        "\n",
        "            # --- DINO-SAM Pipeline ---\n",
        "            try:\n",
        "                image_pil = PIL.Image.open(self.image_path).convert(\"RGB\")\n",
        "\n",
        "                # 1. Get bounding boxes from DINO\n",
        "                bounding_boxes, box_labels = self.dino._get_bounding_box(image_pil, prompt_text)\n",
        "\n",
        "                # 2. Get masks from SAM\n",
        "                if len(bounding_boxes) > 0:\n",
        "                    seg_masks = self.sam._segment(image_pil, bounding_boxes)\n",
        "                else:\n",
        "                    seg_masks = None\n",
        "\n",
        "                # 3. Visualize\n",
        "                Visualize._show_segmentation(image_pil, prompt_text, seg_masks, bounding_boxes, box_labels)\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "    def display_ui(self):\n",
        "        \"\"\"Displays the full user interface.\"\"\"\n",
        "        print(\"Please upload an image to begin.\")\n",
        "        display(self.uploader, self.text_prompt, self.run_button, self.output)\n",
        "\n",
        "\n",
        "\n",
        "# --- Usage ---\n",
        "dino_instance = DINO(mparams=ModelParameters())\n",
        "sam_instance = SAM(mparams=ModelParameters())\n",
        "ui = UI_Handler(dino_model=dino_instance, sam_model=sam_instance)\n",
        "ui.display_ui()"
      ],
      "metadata": {
        "id": "9c1jRupEJGEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHHmZDsqz8Ip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}